{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-llama.ipynb)&nbsp;&nbsp;\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-llama.ipynb)&nbsp;&nbsp;\n",
    "<a href=\"https://raw.githubusercontent.com/pixeltable/pixeltable/release/docs/notebooks/integrations/working-with-llama.ipynb\" download><img src=\"https://img.shields.io/badge/%E2%AC%87-Download%20Notebook-blue\" alt=\"Download Notebook\"></a>\n",
    "\n",
    "# Working with Llama in Pixeltable\n",
    "\n",
    "Pixeltable's Llama integration enables you to access Llama's LLM and other models via the Llama AI API.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- A Llama AI account with an API key (https://llama.developer.meta.com/)\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- Llama usage may incur costs based on your Llama AI plan.\n",
    "- Be mindful of sensitive data and consider security measures when integrating with external services.\n",
    "\n",
    "First you'll need to install required libraries and enter a Llama AI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pip install -qU pixeltable llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if 'LLAMA_API_KEY' not in os.environ:\n",
    "    os.environ['LLAMA_API_KEY'] = getpass.getpass('Llama AI API Key:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a Pixeltable directory to hold the tables for our demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixeltable as pxt\n",
    "\n",
    "# Remove the `llama_demo` directory and its contents, if it exists\n",
    "pxt.drop_dir('llama_demo', force=True)\n",
    "pxt.create_dir('llama_demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "Create a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Llama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.llama import chat_completions\n",
    "\n",
    "# Create a table in Pixeltable and pick a model hosted on Llama with some parameters\n",
    "\n",
    "t = pxt.create_table('llama_demo.chat', {'input': pxt.String})\n",
    "\n",
    "messages = [{'role': 'user', 'content': t.input}]\n",
    "t.add_computed_column(output=chat_completions(\n",
    "    messages=messages,\n",
    "    model='Llama-4-Scout-17B-16E-Instruct-FP8',\n",
    "    # These parameters are optional and can be used to tune model behavior:\n",
    "    max_tokens=300,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the response into a new column\n",
    "t.add_computed_column(response=t.output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a conversation\n",
    "t.insert(input=\"What three species of fish have the highest mercury content?\")\n",
    "t.select(t.input, t.response).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn More\n",
    "\n",
    "To learn more about advanced techniques like RAG operations in Pixeltable, check out the [RAG Operations in Pixeltable](https://pixeltable.readme.io/docs/rag-operations-in-pixeltable) tutorial.\n",
    "\n",
    "If you have any questions, don't hesitate to reach out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
