{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Auto-Generate Video Captions\n",
        "\n",
        "Add subtitles to videos using AI transcription.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%pip install -qU pixeltable openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "if 'OPENAI_API_KEY' not in os.environ:\n",
        "    os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import pixeltable as pxt\n",
        "from pixeltable.functions import openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Step 1: Create table and add video\n",
        "pxt.create_dir('captions', if_exists='ignore')\n",
        "videos = pxt.create_table('captions.videos', {'video': pxt.Video}, if_exists='ignore')\n",
        "videos.insert([{'video': 'https://github.com/pixeltable/pixeltable/raw/release/tests/data/audio/jfk_1961_0109_cityuponahill-excerpt.flac'}])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Extract audio and transcribe\n",
        "videos.add_computed_column(audio=videos.video.extract_audio(), if_exists='ignore')\n",
        "videos.add_computed_column(if_exists='ignore',\n",
        "    transcript=openai.transcriptions(\n",
        "        audio=videos.audio,\n",
        "        model='whisper-1',\n",
        "        model_kwargs={'response_format': 'verbose_json'}\n",
        "    ))\n",
        "videos.add_computed_column(captions=videos.transcript.segments, if_exists='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Step 3: View captions with timestamps\n",
        "result = videos.select(videos.captions).head(1)\n",
        "for segment in result['captions'][0][:3]:  # Show first 3 segments\n",
        "    print(f\"[{segment['start']:.2f}s - {segment['end']:.2f}s]: {segment['text']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What's Happening:**\n",
        "- Video → audio extraction (automatic)\n",
        "- Audio → transcription with timestamps\n",
        "- Segments contain start/end times + text\n",
        "- Perfect for SRT/VTT caption file generation\n",
        "\n",
        "**Variation:** Export to SRT format:\n",
        "```python\n",
        "@pxt.udf\n",
        "def to_srt(segments: list[dict]) -> str:\n",
        "    lines = []\n",
        "    for i, seg in enumerate(segments, 1):\n",
        "        start = format_timestamp(seg['start'])\n",
        "        end = format_timestamp(seg['end'])\n",
        "        lines.append(f\"{i}\\n{start} --> {end}\\n{seg['text']}\\n\")\n",
        "    return '\\n'.join(lines)\n",
        "```\n",
        "\n",
        "**Next:** `index-video-meetings-for-search.ipynb` • `extract-insights-from-earnings-calls.ipynb`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
