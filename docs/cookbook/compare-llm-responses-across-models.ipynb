{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compare LLM Responses Across Models\n",
        "\n",
        "A/B test prompts across multiple AI models side-by-side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%pip install -qU pixeltable anthropic openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "if 'OPENAI_API_KEY' not in os.environ:\n",
        "    os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
        "if 'ANTHROPIC_API_KEY' not in os.environ:\n",
        "    os.environ['ANTHROPIC_API_KEY'] = getpass.getpass('Anthropic API Key:')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import pixeltable as pxt\n",
        "from pixeltable.functions import openai, anthropic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Step 1: Create comparison table\n",
        "pxt.create_dir('compare', if_exists='ignore')\n",
        "prompts = pxt.create_table('compare.prompts', {'prompt': pxt.String}, if_exists='ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Add responses from multiple models\n",
        "msgs = [{'role': 'user', 'content': prompts.prompt}]\n",
        "\n",
        "prompts.add_computed_column(if_exists='ignore',\n",
        "    gpt4=openai.chat_completions(\n",
        "        model='gpt-4o-mini', messages=msgs\n",
        "    ).choices[0].message.content)\n",
        "\n",
        "prompts.add_computed_column(if_exists='ignore',\n",
        "    claude=anthropic.messages(\n",
        "        model='claude-3-haiku-20240307', max_tokens=300, messages=msgs\n",
        "    ).content[0].text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Step 3: Compare responses\n",
        "prompts.insert([{'prompt': 'Explain quantum computing in one sentence'}])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View results side-by-side\n",
        "prompts.select(prompts.prompt, prompts.gpt4, prompts.claude).head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What's Happening:**\n",
        "- Single prompt runs through multiple models\n",
        "- Results stored in separate columns\n",
        "- Easy to compare quality/style/cost\n",
        "- Add more models as columns\n",
        "\n",
        "**Variation:** Add model scoring:\n",
        "```python\n",
        "@pxt.udf\n",
        "def rate_response(text: str) -> float:\n",
        "    # Custom scoring logic\n",
        "    return len(text) / 100  # Simple length score\n",
        "\n",
        "prompts.add_computed_column(gpt4_score=rate_response(prompts.gpt4))\n",
        "```\n",
        "\n",
        "**Next:** `analyze-financial-data-with-llms.ipynb` â€¢ `classify-customer-support-tickets.ipynb`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
